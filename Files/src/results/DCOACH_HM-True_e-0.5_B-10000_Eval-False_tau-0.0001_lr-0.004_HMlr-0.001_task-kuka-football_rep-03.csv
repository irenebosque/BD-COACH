,0,1,2,3,4,5,6,7
Accumulated time steps,0.0,112.0,199.0,242.0,319.0,396.0,459.0,532.0
Episode reward,0.0,0.6542871946953382,0.5823474713659983,0.32650503321080154,0.5223240502578262,0.5483500453380683,0.45676874856130584,0.38049192792655495
Episode feedback,0.8,0.8363636287603307,0.9069767336398055,0.9761904529478465,0.921052619459834,0.9473684085872578,0.9354838558792927,0.0
total seconds,0.0,24.23269748687744,46.49974584579468,64.02072620391846,86.3683590888977,108.860755443573,129.21391105651855,147.9696147441864
total minutes,0.0,0.40387829144795734,0.774995764096578,1.067012103398641,1.4394726514816285,1.8143459240595499,2.153565184275309,2.46616024573644
cummulative feedback,0.0,92.0,170.0,211.0,281.0,353.0,411.0,411.0
e,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5
buffer size,10000.0,10000.0,10000.0,10000.0,10000.0,10000.0,10000.0,10000.0
human model,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0
tau,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001
total_success,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
total_success_div_episode,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
total_policy_loss_agent,0.0,0.0,0.0,3.584415964041909e-08,0.06250004470348358,0.04687502980232239,0.04687502235174179,0.04687502235174179
total_policy_loss_hm,0.0,0.0008193061221390963,0.001211028080433607,0.006331026088446379,0.007123471237719059,0.0038263860624283552,0.0032754831481724977,0.002807898446917534
success_this_episode,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
timesteps_this_episode,0.0,110.0,86.0,42.0,76.0,76.0,62.0,72.0
