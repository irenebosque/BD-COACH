,0
Accumulated time steps,"[0, 502, 1003, 1504, 1597, 1798]"
Episode reward,"[0, 153.51552285607866, 362.8103264249491, 196.72541290576095, 183.98346874250265, 196.0845683565922]"
Episode feedback,"[0.8, 0.0, 0.0, 0.0, 0.0, 0.0]"
total seconds,"[0, 3.5371222496032715, 7.893906354904175, 12.325600624084473, 13.973601579666138, 16.322153568267822]"
total minutes,"[0, 0.05895203749338786, 0.1315651059150696, 0.20542667706807455, 0.2328933596611023, 0.2720358928044637]"
cummulative feedback,"[0, 0, 0, 0, 0, 0]"
e,"[0.1, 0.1, 0.1, 0.1, 0.1, 0.1]"
buffer size,"[6000, 6000, 6000, 6000, 6000, 6000]"
human model,"[False, False, False, False, False, False]"
tau,"[0.00015, 0.00015, 0.00015, 0.00015, 0.00015, 0.00015]"
total_success,"[0, 0, 0, 0, 1, 2]"
total_success_div_episode,"[0, 0.0, 0.0, 0.0, 0.25, 0.4]"
total_policy_loss_agent,"[0, 0, 0, 0, 0, 0]"
total_policy_loss_hm,"[0, 0, 0, 0, 0, 0]"
success_this_episode,1
