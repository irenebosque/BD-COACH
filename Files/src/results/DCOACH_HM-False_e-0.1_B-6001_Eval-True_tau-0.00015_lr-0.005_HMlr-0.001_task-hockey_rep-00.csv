,0,1,2,3,4,5,6,7
Accumulated time steps,0.0,502.0,1003.0,1504.0,1597.0,1798.0,2299.0,2404.0
Episode reward,0.0,153.51552285607866,362.8103264249491,196.72541290576095,183.98346874250265,196.0845683565922,207.8733041713192,126.48119403490277
Episode feedback,0.8,0.0,0.0,0.0,0.0,0.0,0.0,0.0
total seconds,0.0,3.4911510944366455,7.833948850631714,12.226897239685059,13.8560152053833,16.195501565933228,20.646873235702515,22.346405029296875
total minutes,0.0,0.05818585157394409,0.13056581417719523,0.20378162066141764,0.23093358675638834,0.26992502609888713,0.34411455392837526,0.3724400838216146
cummulative feedback,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
e,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1
buffer size,6001.0,6001.0,6001.0,6001.0,6001.0,6001.0,6001.0,6001.0
human model,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
tau,0.00015,0.00015,0.00015,0.00015,0.00015,0.00015,0.00015,0.00015
total_success,0.0,0.0,0.0,0.0,1.0,2.0,2.0,3.0
total_success_div_episode,0.0,0.0,0.0,0.0,0.25,0.4,0.3333333333333333,0.42857142857142855
total_policy_loss_agent,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
total_policy_loss_hm,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
success_this_episode,0.0,0.0,0.0,0.0,1.0,1.0,0.0,1.0
