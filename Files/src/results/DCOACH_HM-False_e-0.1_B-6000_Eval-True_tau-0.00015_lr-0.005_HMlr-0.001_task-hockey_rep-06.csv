,0
Accumulated time steps,"[0, 502, 1003, 1504, 1597, 1798]"
Episode reward,"[0, 153.51552285607866, 362.8103264249491, 196.72541290576095, 183.98346874250265, 196.0845683565922]"
Episode feedback,"[0.8, 0.0, 0.0, 0.0, 0.0, 0.0]"
total seconds,"[0, 3.5600035190582275, 7.980124473571777, 12.496677875518799, 14.133647441864014, 16.516018390655518]"
total minutes,"[0, 0.05933339198430379, 0.13300207455952961, 0.20827796459197997, 0.23556079069773356, 0.27526697317759197]"
cummulative feedback,"[0, 0, 0, 0, 0, 0]"
e,"[0.1, 0.1, 0.1, 0.1, 0.1, 0.1]"
buffer size,"[6000, 6000, 6000, 6000, 6000, 6000]"
human model,"[False, False, False, False, False, False]"
tau,"[0.00015, 0.00015, 0.00015, 0.00015, 0.00015, 0.00015]"
total_success,"[0, 0, 0, 0, 1, 2]"
total_success_div_episode,"[0, 0.0, 0.0, 0.0, 0.25, 0.4]"
total_policy_loss_agent,"[0, 0, 0, 0, 0, 0]"
total_policy_loss_hm,"[0, 0, 0, 0, 0, 0]"
success_this_episode,"[0, 0, 0, 1, 1]"
