,0
Accumulated time steps,"[0, 53, 119, 176, 243]"
Episode reward,"[0, 117.26805547221488, 133.7236967541675, 129.1015563312528, 135.78419282282405]"
Episode feedback,"[0.8, 0.6862744963475589, 0.7538461422485209, 0.57142856122449, 0.6666666565656567]"
total seconds,"[0, 2.5586745738983154, 6.93377685546875, 10.688156843185425, 14.870574235916138]"
total minutes,"[0, 0.042644576231638594, 0.11556294759114584, 0.17813594738642374, 0.24784290393193562]"
cummulative feedback,"[0, 35, 84, 116, 160]"
e,"[0.01, 0.01, 0.01, 0.01, 0.01]"
buffer size,"[3000, 3000, 3000, 3000, 3000]"
human model,"[True, True, True, True, True]"
tau,"[0.0003011, 0.0003011, 0.0003011, 0.0003011, 0.0003011]"
total_success,"[0, 1, 2, 3, 4]"
total_success_div_episode,"[0, 1.0, 1.0, 1.0, 1.0]"
total_policy_loss_agent,"[1.6666634e-05, 1.6666685e-05, 1.6666685e-05, 1.6666634e-05]"
total_policy_loss_hm,"[0.09035187, 0.074141555, 0.22176392, 0.1994778]"
