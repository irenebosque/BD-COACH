,0
Episode,"[0, 1, 0, 1, 5, 4, 5, 4, 10, 7, 10, 7, 15, 10, 15, 10]"
Accumulated time steps,"[0, 2, 2, 1502, 1502, 3002, 3002, 4502, 4502]"
Episode reward,"[0, 230.0605874864175, 196.54435268876043, 123.35436868262548, 118.58923104895126, 736.0583147662929, 222.50335979184433, 171.14345837802176, 166.72921998411402]"
Episode feedback,"[0.9, 0.0, 0.0, 0.001999999996, 0.0, 0.001999999996, 0.0, 0.001999999996, 0.0]"
total seconds,"[0, 1.1250548362731934, 2.21566104888916, 10.776000261306763, 11.988216161727905, 20.270954132080078, 20.6147518157959, 29.338898181915283, 30.530815601348877]"
total minutes,"[0, 0.018750913937886558, 0.036927684148152666, 0.17960000435511272, 0.1998036026954651, 0.33784923553466795, 0.34357919692993166, 0.4889816363652547, 0.508846926689148]"
cummulative feedback,"[0, 0, 0, 1, 1, 2, 2, 3, 3]"
e,"[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]"
buffer size,"[500, 500, 500, 500, 500, 500, 500, 500, 500]"
human model,"[True, True, True, True, True, True, True, True, True]"
tau,"[5e-06, 5e-06, 5e-06, 5e-06, 5e-06, 5e-06, 5e-06, 5e-06, 5e-06]"
total_success,"[0, 0, 0, 1313, 1313, 2628, 2629, 3924, 3924]"
total_success_div_episode,"[0, 0.0, 0.0, 328.25, 328.25, 375.42857142857144, 375.57142857142856, 392.4, 392.4]"
success_this_episode,"[0, 0, 0, 0, 0, 0, 1, 0, 0]"
timesteps_this_episode,"[0, 500, 500, 500, 500, 500, 87, 500, 500]"
