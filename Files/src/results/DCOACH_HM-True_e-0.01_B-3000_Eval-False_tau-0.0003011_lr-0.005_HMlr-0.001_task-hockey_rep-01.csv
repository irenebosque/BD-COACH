,0
Accumulated time steps,"[0, 50, 110, 168, 227, 316]"
Episode reward,"[0, 114.01503381625405, 130.14312861296978, 127.11754872862849, 127.00334859568244, 177.7297105256609]"
Episode feedback,"[0.8, 0.6249999869791669, 0.8135593082447575, 0.7894736703601111, 0.5172413703923902, 0.7386363552427687]"
total seconds,"[0, 2.3532745838165283, 6.7221033573150635, 10.764461040496826, 14.426913022994995, 21.54094123840332]"
total minutes,"[0, 0.03922124306360881, 0.11203505595525105, 0.17940768400828044, 0.24044855038324991, 0.359015687306722]"
cummulative feedback,"[0, 30, 78, 123, 153, 218]"
e,"[0.01, 0.01, 0.01, 0.01, 0.01, 0.01]"
buffer size,"[3000, 3000, 3000, 3000, 3000, 3000]"
human model,"[True, True, True, True, True, True]"
tau,"[0.0003011, 0.0003011, 0.0003011, 0.0003011, 0.0003011, 0.0003011]"
total_success,"[0, 1, 2, 3, 4, 5]"
total_success_div_episode,"[0, 1.0, 1.0, 1.0, 1.0, 1.0]"
total_policy_loss_agent,"[<tf.Tensor: shape=(), dtype=float32, numpy=1.6666634e-05>, <tf.Tensor: shape=(), dtype=float32, numpy=1.6666685e-05>, <tf.Tensor: shape=(), dtype=float32, numpy=3.3333297e-05>, <tf.Tensor: shape=(), dtype=float32, numpy=1.6666672e-05>, <tf.Tensor: shape=(), dtype=float32, numpy=3.3333297e-05>]"
total_policy_loss_hm,"[<tf.Tensor: shape=(), dtype=float32, numpy=0.122562595>, <tf.Tensor: shape=(), dtype=float32, numpy=0.13137962>, <tf.Tensor: shape=(), dtype=float32, numpy=0.22612025>, <tf.Tensor: shape=(), dtype=float32, numpy=0.07950836>, <tf.Tensor: shape=(), dtype=float32, numpy=0.23240967>]"
