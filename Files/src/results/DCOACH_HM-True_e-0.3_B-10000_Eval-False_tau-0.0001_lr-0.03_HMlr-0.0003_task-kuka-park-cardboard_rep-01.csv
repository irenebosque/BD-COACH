,0,1,2,3,4,5
Accumulated time steps,0.0,2.0,138.0,179.0,213.0,258.0
Episode reward,0.0,0.0,81.45258769087421,26.214196842236184,34.8848053286629,36.016018877055885
Episode feedback,0.8,0.0,0.6296296249657065,0.8499999787500006,0.6969696758494038,0.9318181606404964
total seconds,0.0,10.204728603363037,38.63467764854431,55.39440989494324,72.02212452888489,90.9634039402008
total minutes,0.0,0.1700788100560506,0.6439112941424052,0.9232401649157206,1.2003687421480815,1.51605673233668
cummulative feedback,0.0,0.0,85.0,119.0,142.0,183.0
e,0.3,0.3,0.3,0.3,0.3,0.3
buffer size,10000.0,10000.0,10000.0,10000.0,10000.0,10000.0
human model,1.0,1.0,1.0,1.0,1.0,1.0
tau,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001
total_success,0.0,1.0,2.0,2.0,2.0,2.0
total_success_div_episode,0.0,1.0,1.0,0.6666666666666666,0.5,0.4
total_policy_loss_agent,0.0,0.0,0.0,0.0,0.005747633520513773,0.007426941767334938
total_policy_loss_hm,0.0,0.0,0.08030134439468384,0.09585191309452057,0.14722254872322083,0.19062119722366333
success_this_episode,0.0,1.0,1.0,0.0,0.0,0.0
timesteps_this_episode,0.0,0.0,135.0,40.0,33.0,44.0
