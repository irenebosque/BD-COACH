,0,1,2,3,4,5,6
Accumulated time steps,0.0,112.0,343.0,704.0,790.0,852.0,962.0
Episode reward,0.0,85.93020640583273,191.5731653800208,178.19341154525915,90.20255693247438,80.85157993488247,125.58774375060041
Episode feedback,0.8,0.7363636296694216,0.930434778563327,0.49166666530092595,0.941176459515571,0.8524590024187049,0.9357798079286256
total seconds,0.0,23.657833099365234,64.6304223537445,121.87253761291504,145.49814343452454,165.4266676902771,192.66986060142517
total minutes,0.0,0.3942972183227539,1.0771737058957418,2.0312089602152508,2.4249690572420755,2.757111128171285,3.211164343357086
cummulative feedback,0.0,81.0,295.0,472.0,552.0,604.0,706.0
e,0.5,0.5,0.5,0.5,0.5,0.5,0.5
buffer size,10000.0,10000.0,10000.0,10000.0,10000.0,10000.0,10000.0
human model,1.0,1.0,1.0,1.0,1.0,1.0,1.0
tau,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001
total_success,0.0,0.0,1.0,2.0,3.0,3.0,3.0
total_success_div_episode,0.0,0.0,0.5,0.6666666666666666,0.75,0.6,0.5
total_policy_loss_agent,0.0,0.0,0.0,0.023465264588594437,0.008798518218100071,0.0004821007896680385,0.02457001805305481
total_policy_loss_hm,0.0,0.25087088346481323,0.09722258150577545,0.22419032454490662,0.21804435551166534,0.1323494166135788,0.16107940673828125
success_this_episode,0.0,0.0,1.0,1.0,1.0,0.0,0.0
timesteps_this_episode,0.0,110.0,230.0,360.0,85.0,61.0,109.0
