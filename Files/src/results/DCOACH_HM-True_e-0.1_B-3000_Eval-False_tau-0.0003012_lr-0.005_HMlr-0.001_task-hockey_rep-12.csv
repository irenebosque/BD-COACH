,0,1,2,3,4,5,6,7
Accumulated time steps,0.0,71.0,131.0,193.0,377.0,499.0,608.0,670.0
Episode reward,0.0,140.69679170050392,129.6972128546743,130.70616380488295,351.31616148130956,245.5566886195689,204.36180746206972,147.06698508531025
Episode feedback,0.8,0.7826086843100191,0.6779660902039646,0.5737704823972052,0.7923497224461764,0.710743795778977,0.6666666604938273,0.7377049059392639
total seconds,0.0,4.205312252044678,8.503409147262573,12.680768728256226,28.294117212295532,38.360769271850586,47.69960021972656,53.394280195236206
total minutes,0.0,0.07008853753407797,0.14172348578770955,0.21134614547093708,0.4715686202049255,0.6393461545308431,0.7949933369954427,0.8899046699206035
cummulative feedback,0.0,54.0,94.0,129.0,274.0,360.0,432.0,477.0
e,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1
buffer size,3000.0,3000.0,3000.0,3000.0,3000.0,3000.0,3000.0,3000.0
human model,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0
tau,0.0003012,0.0003012,0.0003012,0.0003012,0.0003012,0.0003012,0.0003012,0.0003012
total_success,0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0
total_success_div_episode,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0
total_policy_loss_agent,0.0,0.0,0.0,0.0,0.003750000149011612,0.0040300446562469006,0.004249767400324345,0.004180323798209429
total_policy_loss_hm,0.0,0.0,0.0,0.25795090198516846,0.15270213782787323,0.22524245083332062,0.27508124709129333,0.13307224214076996
