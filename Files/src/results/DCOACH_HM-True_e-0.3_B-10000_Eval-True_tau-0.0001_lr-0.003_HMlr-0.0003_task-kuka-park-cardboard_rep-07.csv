,0,1,2,3,4,5,6,7,8,9
Accumulated time steps,0.0,152.0,328.0,472.0,473.0,614.0,1115.0,1163.0,1225.0,1410.0
Episode reward,0.0,74.16885622075306,44.43353472732973,69.73982271045617,0.0,66.82663106446029,17.15544859890404,26.940086862292603,32.96038892080425,63.72213475524275
Episode feedback,0.8,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
total seconds,0.0,25.645233154296875,54.834280252456665,80.68683695793152,91.80118989944458,117.35643458366394,179.99663281440735,195.96021175384521,213.37186694145203,243.45481419563293
total minutes,0.0,0.4274205525716146,0.9139046708742777,1.3447806159655253,1.5300198316574096,1.955940576394399,2.9999438802401226,3.2660035292307534,3.556197782357534,4.057580236593882
cummulative feedback,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
e,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3
buffer size,10000.0,10000.0,10000.0,10000.0,10000.0,10000.0,10000.0,10000.0,10000.0,10000.0
human model,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0
tau,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001
total_success,0.0,1.0,1.0,2.0,3.0,4.0,4.0,4.0,4.0,4.0
total_success_div_episode,0.0,1.0,0.5,0.6666666666666666,0.75,0.8,0.6666666666666666,0.5714285714285714,0.5,0.4444444444444444
total_policy_loss_agent,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
total_policy_loss_hm,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
success_this_episode,0.0,1.0,0.0,1.0,1.0,1.0,0.0,0.0,0.0,0.0
timesteps_this_episode,0.0,150.0,175.0,143.0,0.0,140.0,500.0,47.0,61.0,184.0
