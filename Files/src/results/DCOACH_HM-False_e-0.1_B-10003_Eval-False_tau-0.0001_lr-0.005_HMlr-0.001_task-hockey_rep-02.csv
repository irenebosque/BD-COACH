,0,1,2,3,4,5,6,7,8,9,10,11
Accumulated time steps,0.0,366.0,640.0,988.0,1159.0,1654.0,1859.0,2005.0,2157.0,2330.0,2480.0,2746.0
Episode reward,0.0,532.2777516597412,540.3777335947715,623.523120630456,335.2798497638224,857.7252022787119,405.19374805468334,314.12914903835434,350.55555630429836,323.29829027036953,352.444754876236,495.6113500836141
Episode feedback,0.8,0.7747252725969086,0.816849813857693,0.7521613811176906,0.7235294075086506,0.7105263143511613,0.7401960748029605,0.6689655126278241,0.5827814530941626,0.6744186007301244,0.6375838883383632,0.6490566013243148
total seconds,0.0,12.075496912002563,22.19055986404419,34.449267864227295,40.543702125549316,55.93346452713013,63.15782117843628,68.43563938140869,73.34136152267456,79.14939284324646,84.22079753875732,93.08360314369202
total minutes,0.0,0.20125828186670938,0.3698426644007365,0.5741544644037883,0.6757283687591553,0.9322244087855022,1.052630352973938,1.140593989690145,1.2223560253779093,1.319156547387441,1.4036799589792888,1.5513933857282003
cummulative feedback,0.0,282.0,505.0,766.0,889.0,1240.0,1391.0,1488.0,1576.0,1692.0,1787.0,1959.0
e,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1
buffer size,10003.0,10003.0,10003.0,10003.0,10003.0,10003.0,10003.0,10003.0,10003.0,10003.0,10003.0,10003.0
human model,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
tau,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001
total_success,0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0
total_success_div_episode,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0
total_policy_loss_agent,0.0,0.06684642285108566,0.026879915967583656,0.11891857534646988,0.012587782926857471,0.025460565462708473,0.017278948798775673,0.032434236258268356,0.03318400681018829,0.024828052148222923,0.0260922908782959,0.025189943611621857
total_policy_loss_hm,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
success_this_episode,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0
