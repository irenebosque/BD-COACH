,0,1,2,3,4
Accumulated time steps,0.0,63.0,130.0,194.0,268.0
Episode reward,0.0,144.09407834482812,147.72641256201817,131.9616606187483,200.482745181386
Episode feedback,0.9,0.499999991935484,0.6865671539318335,0.8437499868164064,0.8108107998539081
total seconds,0.0,0.8537569046020508,2.20920991897583,3.702294111251831,6.2729456424713135
total minutes,0.0,0.014229281743367513,0.036820165316263836,0.061704901854197185,0.10454909404118856
cummulative feedback,0.0,31.0,77.0,131.0,191.0
e,0.1,0.1,0.1,0.1,0.1
buffer size,500.0,500.0,500.0,500.0,500.0
human model,1.0,1.0,1.0,1.0,1.0
tau,5e-06,5e-06,5e-06,5e-06,5e-06
total_success,0.0,1.0,2.0,3.0,4.0
total_success_div_episode,0.0,1.0,1.0,1.0,1.0
total_policy_loss_agent,0.0,0.0,0.0,0.0,0.000561022839974612
total_policy_loss_hm,0.0,0.15907782316207886,0.2030322551727295,0.21430064737796783,0.2823552191257477
success_this_episode,0.0,1.0,1.0,1.0,1.0
timesteps_this_episode,0.0,62.0,67.0,64.0,74.0
