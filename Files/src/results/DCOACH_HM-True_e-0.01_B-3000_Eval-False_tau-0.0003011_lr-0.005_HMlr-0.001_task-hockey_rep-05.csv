,0,1,2,3,4,5,6,7,8
Accumulated time steps,0.0,62.0,120.0,182.0,243.0,306.0,388.0,513.0,599.0
Episode reward,0.0,130.4091140436013,125.3815430587425,131.97688413362266,132.03825454705088,132.44279355388005,168.80430261854835,227.01044923684594,180.30959199724265
Episode feedback,0.8,0.8166666530555559,0.43859648353339503,0.5081967129803818,0.5499999908333335,0.4838709599375652,0.8271604836153027,0.6612903172476587,0.6941176388927337
total seconds,0.0,3.2521777153015137,6.606403589248657,10.432965993881226,14.133635759353638,17.826234340667725,25.10234022140503,34.078843116760254,40.872122287750244
total minutes,0.0,0.0542029619216919,0.11010672648747762,0.1738827665646871,0.2355605959892273,0.29710390567779543,0.41837233702341714,0.5679807186126709,0.6812020381291707
cummulative feedback,0.0,49.0,74.0,105.0,138.0,168.0,235.0,317.0,376.0
e,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01
buffer size,3000.0,3000.0,3000.0,3000.0,3000.0,3000.0,3000.0,3000.0,3000.0
human model,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0
tau,0.0003011,0.0003011,0.0003011,0.0003011,0.0003011,0.0003011,0.0003011,0.0003011,0.0003011
total_success,0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0
total_success_div_episode,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0
total_policy_loss_agent,0.0,1.666663411015179e-05,3.333326822030358e-05,1.666663411015179e-05,1.666663411015179e-05,1.666663411015179e-05,3.3333300962112844e-05,3.333335553179495e-05,1.6666666851961054e-05
total_policy_loss_hm,0.0,0.17835550010204315,0.20672045648097992,0.14523817598819733,0.13939853012561798,0.18096089363098145,0.18934722244739532,0.17786002159118652,0.2555582821369171
