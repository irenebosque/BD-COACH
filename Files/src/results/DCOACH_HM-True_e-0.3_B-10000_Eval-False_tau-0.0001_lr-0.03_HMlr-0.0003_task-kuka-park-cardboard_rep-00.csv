,0,1,2,3
Accumulated time steps,0.0,361.0,455.0,640.0
Episode reward,0.0,126.29599393614308,74.72456575363044,136.4348010064395
Episode feedback,0.8,0.31754874563356894,0.6989247236674762,0.7010869527114839
total seconds,0.0,52.10671639442444,79.04013156890869,121.30291438102722
total minutes,0.0,0.8684452732404073,1.3173355261484783,2.021715239683787
cummulative feedback,0.0,114.0,179.0,308.0
e,0.3,0.3,0.3,0.3
buffer size,10000.0,10000.0,10000.0,10000.0
human model,1.0,1.0,1.0,1.0
tau,0.0001,0.0001,0.0001,0.0001
total_success,0.0,1.0,2.0,2.0
total_success_div_episode,0.0,1.0,1.0,0.6666666666666666
total_policy_loss_agent,0.0,0.0,7.87732788012363e-05,0.0006572343991138041
total_policy_loss_hm,0.0,0.14719326794147491,0.14318448305130005,0.1210281103849411
success_this_episode,0.0,1.0,1.0,0.0
timesteps_this_episode,0.0,359.0,93.0,184.0
