,0,1,2,3,4,5,6,7,8,9
Accumulated time steps,0.0,656.0,792.0,949.0,1154.0,1263.0,1552.0,1740.0,1811.0,2050.0
Episode reward,0.0,89.58373957872391,46.60116961598396,54.46719563007355,71.89144107699394,39.367776334285736,49.36529180407524,56.527204126119614,24.93531909584999,65.56213282048702
Episode feedback,0.8,0.7445255447280089,0.4148148117421125,0.7051282006081526,0.6078431342752788,0.38888888528806587,0.3214285691326531,0.572192510309131,0.5571428491836736,0.6302520981922181
total seconds,0.0,99.47153806686401,126.25494384765625,156.76178526878357,192.6636734008789,216.26836729049683,270.93515062332153,304.69776916503906,324.26849579811096,364.44127583503723
total minutes,0.0,1.657858967781067,2.1042490641276044,2.612696421146393,3.211061223347982,3.604472788174947,4.515585843722025,5.078296152750651,5.404474929968516,6.074021263917287
cummulative feedback,0.0,204.0,260.0,370.0,494.0,536.0,581.0,688.0,727.0,877.0
e,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3
buffer size,1500.0,1500.0,1500.0,1500.0,1500.0,1500.0,1500.0,1500.0,1500.0,1500.0
human model,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
tau,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001,0.0001
total_success,0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0
total_success_div_episode,0.0,0.5,0.6666666666666666,0.75,0.8,0.8333333333333334,0.75,0.7777777777777778,0.8,0.8181818181818182
total_policy_loss_agent,0.0,0.1769912838935852,0.09787929058074951,0.17965342104434967,0.06108405441045761,0.15111622214317322,0.03177177533507347,0.08795531094074249,0.097747802734375,0.12152256071567535
total_policy_loss_hm,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
