,0
Accumulated time steps,"[0, 502, 1003, 1504, 1597, 1798, 2299, 2404]"
Episode reward,"[0, 153.51552285607866, 362.8103264249491, 196.72541290576095, 183.98346874250265, 196.0845683565922, 207.8733041713192, 126.48119403490277]"
Episode feedback,"[0.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]"
total seconds,"[0, 4.1156182289123535, 8.487536430358887, 12.957568168640137, 14.610055208206177, 16.958780527114868, 21.397677898406982, 23.132465362548828]"
total minutes,"[0, 0.06859363714853922, 0.14145894050598146, 0.2159594694773356, 0.2435009201367696, 0.28264634211858114, 0.35662796497344973, 0.38554108937581383]"
cummulative feedback,"[0, 0, 0, 0, 0, 0, 0, 0]"
e,"[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]"
buffer size,"[6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000]"
human model,"[False, False, False, False, False, False, False, False]"
tau,"[0.00015, 0.00015, 0.00015, 0.00015, 0.00015, 0.00015, 0.00015, 0.00015]"
total_success,"[0, 0, 0, 0, 1, 2, 2, 3]"
total_success_div_episode,"[0, 0.0, 0.0, 0.0, 0.25, 0.4, 0.3333333333333333, 0.42857142857142855]"
total_policy_loss_agent,"[0, 0, 0, 0, 0, 0, 0, 0]"
total_policy_loss_hm,"[0, 0, 0, 0, 0, 0, 0, 0]"
success_this_episode,1
