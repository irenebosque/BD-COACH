,0,1,2,3,4,5
Accumulated time steps,0.0,502.0,608.0,726.0,860.0,989.0
Episode reward,0.0,715.3889700692268,280.29316649806634,295.86563662634245,317.6182090444911,320.8208959938183
Episode feedback,0.8,0.761999998476,0.6952380886167802,0.666666660968661,0.7443608966589407,0.703124994506836
total seconds,0.0,14.274417400360107,18.091514825820923,22.222912073135376,27.056727409362793,31.462656259536743
total minutes,0.0,0.23790695667266845,0.3015252470970154,0.3703818678855896,0.4509454568227132,0.5243776043256124
cummulative feedback,0.0,381.0,454.0,532.0,631.0,721.0
e,0.1,0.1,0.1,0.1,0.1,0.1
buffer size,6001.0,6001.0,6001.0,6001.0,6001.0,6001.0
human model,0.0,0.0,0.0,0.0,0.0,0.0
tau,0.00015,0.00015,0.00015,0.00015,0.00015,0.00015
total_success,0.0,0.0,1.0,2.0,3.0,4.0
total_success_div_episode,0.0,0.0,0.5,0.6666666666666666,0.75,0.8
total_policy_loss_agent,0.0,0.08531977981328964,0.05303892493247986,0.03340774402022362,0.05152906849980354,0.016932548955082893
total_policy_loss_hm,0.0,0.0,0.0,0.0,0.0,0.0
success_this_episode,0.0,0.0,1.0,1.0,1.0,1.0
