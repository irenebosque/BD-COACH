[GENERAL]
render_delay = 0.035
save_results = True
evaluation = False
max_num_of_episodes = 1000
max_time_steps_episode = 1e13
environment = MountainCarContinuous-v0
count_down = False
render = True
graph_folder_path = graphs/mountain_car_DCOACH_full/
eval_save_path = results/mountain_car_DCOACH_full/
; These next parameters are used for the Oracle to decide when to give feedback
; tau = rate of decay
tau = 0.00045
; alpha = Initial percentage of the episode that the oracle gives feedback
alpha = 0.9
; theta = threshold that decides if the different between a_agent and a_teacher is big enough to give feedback
theta = 0.1

[TRANSITION_MODEL]
transition_model = full
image_side_length= 64
buffer_max_size = 100000
buffer_min_size = 100
buffer_sampling_rate = 1000000000
train_end_episode = True
buffer_sampling_size = 20
lstm_hidden_state_size = 150
training_sequence_length = 10
number_training_iterations = 20
learning_rate = 0.0005
show_observation = True
show_transition_model_output = True
resize_observation = True
crop_observation = True
occlude_observation = False
save_transitions = True
save_transition_model = False
load_transition_model = False

[AGENT]
agent = DCOACH

buffer_min_size = 20
buffer_sampling_rate = 10
train_end_episode = False
buffer_sampling_size = 8
dim_a = 3
action_upper_limits = 1
action_lower_limits = -1
; Next e and buffer size, only used in MANUAL tests
e = 1
buffer_max_size = 100
human_model_included = True
; Next learning rate is NOT used, see lines 49 and 50
learning_rate = 0.003
save_policy = False
load_policy = False
; 8/5/21: policy_lr was 0.0075 and human lr was 0.007
policy_model_learning_rate = 0.0075
human_model_learning_rate = 0.005


[FEEDBACK]
key_type = 1
h_up = 0
h_down = 0
h_right = 1
h_left = -1
h_null = 0
